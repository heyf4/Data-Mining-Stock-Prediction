{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task1_lstm_good.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMrmSk0ne7v",
        "colab_type": "text"
      },
      "source": [
        "#### 1. load the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSCuBYGnnN9D",
        "colab_type": "code",
        "outputId": "9df9e1f3-a8d1-4595-b3fa-58a2069761ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1RayMjAmn0PhXM0I-bypjFBwU2RtgDwSw'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('data.zip') \n",
        "\n",
        "\n",
        "path = os.listdir('.')\n",
        "print (path)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\",\"r\") as datazip:\n",
        "  datazip.extractall(\"\")\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"data.csv\") "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 11.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "['.config', 'data.zip', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awvI0Wi3BU9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c2e995b-f20e-43de-c818-79cd866365fc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0I71uDRnhFU",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Normalization\n",
        "是否要加上时间戳？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX7fFFzgIBAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.iloc[:100000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1Pl0tU1qVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.drop(['UpdateTime', 'UpdateMillisec'],axis=1)\n",
        "data_norm=(data - data.mean()) / (data.max() - data.min())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbwxtit11Pn",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Build training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjenS_-MwSVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some parameters\n",
        "n_samples=len(data)\n",
        "ratio=0.3\n",
        "past_n=50\n",
        "future_n=10\n",
        "# droplist=['midPrice', 'AskPrice1','BidPrice1','AskVolume1','BidVolume1']\n",
        "\n",
        "\n",
        "def buildTrain(data,past_n,future_n):\n",
        "  X_set=[]\n",
        "  Y_set=[]\n",
        "  n_samples=len(data)\n",
        "  for i in range(0,n_samples-past_n-future_n,1): #step=1\n",
        "    X_set.append(np.array(data.iloc[i:i+past_n]))\n",
        "    Y_set.append(np.array(data.iloc[i+past_n+future_n-1][\"midPrice\"]-data.iloc[i+past_n-1][\"midPrice\"]))\n",
        "  return np.array(X_set), np.array(Y_set)\n",
        "\n",
        "\n",
        "def shuffle(X,Y):\n",
        "  np.random.seed(6)\n",
        "  randomList = np.arange(X.shape[0])\n",
        "  np.random.shuffle(randomList)\n",
        "  return X[randomList], Y[randomList]\n",
        "\n",
        "def tt_split(X,Y,ratio):\n",
        "  X_train = X[int(X.shape[0]*ratio):]\n",
        "  Y_train = Y[int(Y.shape[0]*ratio):]\n",
        "  X_test = X[:int(X.shape[0]*ratio)]\n",
        "  Y_test = Y[:int(Y.shape[0]*ratio)]\n",
        "  return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "X_set, Y_set=buildTrain(data_norm, past_n, future_n)\n",
        "\n",
        "X_set, Y_set=shuffle(X_set, Y_set)\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = tt_split(X_set, Y_set, ratio)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCdj6E2J5fo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1053f9ad-9f38-4d2e-9bd2-780e48e90512"
      },
      "source": [
        "print(X_train.shape[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dGUMkcqA67R",
        "colab_type": "text"
      },
      "source": [
        "#### 4. build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsDIZ0CSA6At",
        "colab_type": "code",
        "outputId": "159ea977-9a76-4f09-ddd7-d4bd1b9447e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100, input_length=X_train.shape[1], input_dim=X_train.shape[2]))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(100, input_shape=(50, 137))`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100)               95200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 95,301\n",
            "Trainable params: 95,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ZHp7QrFJ3x",
        "colab_type": "text"
      },
      "source": [
        "#### 5. train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br94NgD2FKNO",
        "colab_type": "code",
        "outputId": "aa41b192-66f8-4eb8-b741-3c4aee908d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3727
        }
      },
      "source": [
        "callback = EarlyStopping(monitor=\"loss\", patience=20, verbose=1, mode=\"auto\")\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=128, validation_data=(X_test, Y_test), callbacks=[callback])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 69958 samples, validate on 29982 samples\n",
            "Epoch 1/100\n",
            "69958/69958 [==============================] - 53s 755us/step - loss: 1.3135e-04 - val_loss: 2.9551e-05\n",
            "Epoch 2/100\n",
            "69958/69958 [==============================] - 49s 695us/step - loss: 2.5905e-05 - val_loss: 1.7643e-05\n",
            "Epoch 3/100\n",
            "69958/69958 [==============================] - 48s 693us/step - loss: 2.1476e-05 - val_loss: 2.0123e-05\n",
            "Epoch 4/100\n",
            "69958/69958 [==============================] - 47s 670us/step - loss: 1.9652e-05 - val_loss: 1.5038e-05\n",
            "Epoch 5/100\n",
            "69958/69958 [==============================] - 48s 690us/step - loss: 2.2024e-05 - val_loss: 1.9749e-05\n",
            "Epoch 6/100\n",
            "69958/69958 [==============================] - 47s 670us/step - loss: 1.9017e-05 - val_loss: 2.2234e-05\n",
            "Epoch 7/100\n",
            "69958/69958 [==============================] - 50s 710us/step - loss: 1.7443e-05 - val_loss: 1.4469e-05\n",
            "Epoch 8/100\n",
            "69958/69958 [==============================] - 49s 700us/step - loss: 1.6982e-05 - val_loss: 1.5763e-05\n",
            "Epoch 9/100\n",
            "69958/69958 [==============================] - 47s 670us/step - loss: 1.6646e-05 - val_loss: 1.4663e-05\n",
            "Epoch 10/100\n",
            "69958/69958 [==============================] - 48s 688us/step - loss: 1.5750e-05 - val_loss: 1.5566e-05\n",
            "Epoch 11/100\n",
            "69958/69958 [==============================] - 47s 669us/step - loss: 1.5636e-05 - val_loss: 1.3987e-05\n",
            "Epoch 12/100\n",
            "69958/69958 [==============================] - 48s 691us/step - loss: 1.4831e-05 - val_loss: 1.5245e-05\n",
            "Epoch 13/100\n",
            "69958/69958 [==============================] - 48s 687us/step - loss: 1.4867e-05 - val_loss: 1.4407e-05\n",
            "Epoch 14/100\n",
            "69958/69958 [==============================] - 49s 698us/step - loss: 1.3644e-05 - val_loss: 1.5001e-05\n",
            "Epoch 15/100\n",
            "69958/69958 [==============================] - 48s 684us/step - loss: 1.3530e-05 - val_loss: 1.2465e-05\n",
            "Epoch 16/100\n",
            "69958/69958 [==============================] - 46s 663us/step - loss: 1.3071e-05 - val_loss: 1.2516e-05\n",
            "Epoch 17/100\n",
            "69958/69958 [==============================] - 48s 682us/step - loss: 1.2696e-05 - val_loss: 1.2319e-05\n",
            "Epoch 18/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 1.2520e-05 - val_loss: 1.2548e-05\n",
            "Epoch 19/100\n",
            "69958/69958 [==============================] - 47s 665us/step - loss: 1.2330e-05 - val_loss: 1.1878e-05\n",
            "Epoch 20/100\n",
            "69958/69958 [==============================] - 48s 692us/step - loss: 1.2238e-05 - val_loss: 1.2227e-05\n",
            "Epoch 21/100\n",
            "69958/69958 [==============================] - 47s 672us/step - loss: 1.2040e-05 - val_loss: 1.1517e-05\n",
            "Epoch 22/100\n",
            "69958/69958 [==============================] - 48s 682us/step - loss: 1.1698e-05 - val_loss: 1.2881e-05\n",
            "Epoch 23/100\n",
            "69958/69958 [==============================] - 47s 673us/step - loss: 1.1724e-05 - val_loss: 1.2440e-05\n",
            "Epoch 24/100\n",
            "69958/69958 [==============================] - 47s 670us/step - loss: 1.1282e-05 - val_loss: 1.1876e-05\n",
            "Epoch 25/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 1.1359e-05 - val_loss: 1.1451e-05\n",
            "Epoch 26/100\n",
            "69958/69958 [==============================] - 46s 663us/step - loss: 1.1105e-05 - val_loss: 1.0771e-05\n",
            "Epoch 27/100\n",
            "69958/69958 [==============================] - 49s 706us/step - loss: 1.0923e-05 - val_loss: 1.1550e-05\n",
            "Epoch 28/100\n",
            "69958/69958 [==============================] - 46s 664us/step - loss: 1.0918e-05 - val_loss: 1.1188e-05\n",
            "Epoch 29/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 1.0554e-05 - val_loss: 1.3212e-05\n",
            "Epoch 30/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 1.0549e-05 - val_loss: 1.0545e-05\n",
            "Epoch 31/100\n",
            "69958/69958 [==============================] - 46s 662us/step - loss: 1.0416e-05 - val_loss: 1.1858e-05\n",
            "Epoch 32/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 1.0300e-05 - val_loss: 1.0225e-05\n",
            "Epoch 33/100\n",
            "69958/69958 [==============================] - 48s 685us/step - loss: 1.0526e-05 - val_loss: 1.0176e-05\n",
            "Epoch 34/100\n",
            "69958/69958 [==============================] - 48s 685us/step - loss: 1.0203e-05 - val_loss: 1.0076e-05\n",
            "Epoch 35/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 1.0311e-05 - val_loss: 1.0336e-05\n",
            "Epoch 36/100\n",
            "69958/69958 [==============================] - 46s 661us/step - loss: 9.9672e-06 - val_loss: 9.6662e-06\n",
            "Epoch 37/100\n",
            "69958/69958 [==============================] - 48s 679us/step - loss: 9.7369e-06 - val_loss: 9.7682e-06\n",
            "Epoch 38/100\n",
            "69958/69958 [==============================] - 46s 664us/step - loss: 9.6215e-06 - val_loss: 9.4442e-06\n",
            "Epoch 39/100\n",
            "69958/69958 [==============================] - 48s 680us/step - loss: 9.5772e-06 - val_loss: 1.0210e-05\n",
            "Epoch 40/100\n",
            "69958/69958 [==============================] - 50s 709us/step - loss: 9.5278e-06 - val_loss: 9.7051e-06\n",
            "Epoch 41/100\n",
            "69958/69958 [==============================] - 47s 666us/step - loss: 9.4551e-06 - val_loss: 9.7764e-06\n",
            "Epoch 42/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 9.3078e-06 - val_loss: 9.3216e-06\n",
            "Epoch 43/100\n",
            "69958/69958 [==============================] - 46s 664us/step - loss: 9.1360e-06 - val_loss: 9.4161e-06\n",
            "Epoch 44/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 8.9745e-06 - val_loss: 9.0585e-06\n",
            "Epoch 45/100\n",
            "69958/69958 [==============================] - 46s 664us/step - loss: 8.7595e-06 - val_loss: 8.7991e-06\n",
            "Epoch 46/100\n",
            "69958/69958 [==============================] - 49s 699us/step - loss: 9.2572e-06 - val_loss: 9.9810e-06\n",
            "Epoch 47/100\n",
            "69958/69958 [==============================] - 48s 680us/step - loss: 8.8536e-06 - val_loss: 8.6847e-06\n",
            "Epoch 48/100\n",
            "69958/69958 [==============================] - 46s 661us/step - loss: 8.4873e-06 - val_loss: 8.9796e-06\n",
            "Epoch 49/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 8.5953e-06 - val_loss: 9.0154e-06\n",
            "Epoch 50/100\n",
            "69958/69958 [==============================] - 46s 663us/step - loss: 8.4671e-06 - val_loss: 8.9495e-06\n",
            "Epoch 51/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 8.3387e-06 - val_loss: 8.7980e-06\n",
            "Epoch 52/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 8.1883e-06 - val_loss: 8.6529e-06\n",
            "Epoch 53/100\n",
            "69958/69958 [==============================] - 48s 685us/step - loss: 8.2319e-06 - val_loss: 8.2349e-06\n",
            "Epoch 54/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 8.0410e-06 - val_loss: 8.6198e-06\n",
            "Epoch 55/100\n",
            "69958/69958 [==============================] - 46s 661us/step - loss: 7.9505e-06 - val_loss: 9.0039e-06\n",
            "Epoch 56/100\n",
            "69958/69958 [==============================] - 48s 679us/step - loss: 7.8592e-06 - val_loss: 7.9754e-06\n",
            "Epoch 57/100\n",
            "69958/69958 [==============================] - 47s 670us/step - loss: 7.7290e-06 - val_loss: 8.0194e-06\n",
            "Epoch 58/100\n",
            "69958/69958 [==============================] - 47s 668us/step - loss: 7.6777e-06 - val_loss: 8.1917e-06\n",
            "Epoch 59/100\n",
            "69958/69958 [==============================] - 49s 694us/step - loss: 7.3751e-06 - val_loss: 8.1508e-06\n",
            "Epoch 60/100\n",
            "69958/69958 [==============================] - 47s 673us/step - loss: 7.7009e-06 - val_loss: 7.5679e-06\n",
            "Epoch 61/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 7.3039e-06 - val_loss: 7.6238e-06\n",
            "Epoch 62/100\n",
            "69958/69958 [==============================] - 46s 661us/step - loss: 7.2551e-06 - val_loss: 7.6670e-06\n",
            "Epoch 63/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 7.0822e-06 - val_loss: 7.6528e-06\n",
            "Epoch 64/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 7.1622e-06 - val_loss: 7.8875e-06\n",
            "Epoch 65/100\n",
            "69958/69958 [==============================] - 46s 660us/step - loss: 6.8435e-06 - val_loss: 7.4170e-06\n",
            "Epoch 66/100\n",
            "69958/69958 [==============================] - 49s 704us/step - loss: 6.7637e-06 - val_loss: 7.1318e-06\n",
            "Epoch 67/100\n",
            "69958/69958 [==============================] - 46s 659us/step - loss: 6.6394e-06 - val_loss: 7.3177e-06\n",
            "Epoch 68/100\n",
            "69958/69958 [==============================] - 48s 680us/step - loss: 6.7765e-06 - val_loss: 7.2982e-06\n",
            "Epoch 69/100\n",
            "69958/69958 [==============================] - 47s 675us/step - loss: 6.5530e-06 - val_loss: 7.2357e-06\n",
            "Epoch 70/100\n",
            "69958/69958 [==============================] - 46s 659us/step - loss: 6.3645e-06 - val_loss: 7.6016e-06\n",
            "Epoch 71/100\n",
            "69958/69958 [==============================] - 47s 676us/step - loss: 6.4411e-06 - val_loss: 6.8879e-06\n",
            "Epoch 72/100\n",
            "69958/69958 [==============================] - 47s 669us/step - loss: 6.3126e-06 - val_loss: 7.0839e-06\n",
            "Epoch 73/100\n",
            "69958/69958 [==============================] - 48s 687us/step - loss: 6.3468e-06 - val_loss: 6.6757e-06\n",
            "Epoch 74/100\n",
            "69958/69958 [==============================] - 46s 660us/step - loss: 6.3714e-06 - val_loss: 6.9534e-06\n",
            "Epoch 75/100\n",
            "69958/69958 [==============================] - 47s 669us/step - loss: 5.9928e-06 - val_loss: 6.5576e-06\n",
            "Epoch 76/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 6.0671e-06 - val_loss: 7.0091e-06\n",
            "Epoch 77/100\n",
            "69958/69958 [==============================] - 46s 659us/step - loss: 6.0281e-06 - val_loss: 6.3359e-06\n",
            "Epoch 78/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 6.0029e-06 - val_loss: 6.5096e-06\n",
            "Epoch 79/100\n",
            "69958/69958 [==============================] - 48s 691us/step - loss: 5.7633e-06 - val_loss: 6.5866e-06\n",
            "Epoch 80/100\n",
            "69958/69958 [==============================] - 47s 677us/step - loss: 5.7836e-06 - val_loss: 6.4216e-06\n",
            "Epoch 81/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 5.7511e-06 - val_loss: 7.4216e-06\n",
            "Epoch 82/100\n",
            "69958/69958 [==============================] - 46s 659us/step - loss: 5.7008e-06 - val_loss: 6.6358e-06\n",
            "Epoch 83/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 5.7870e-06 - val_loss: 6.2098e-06\n",
            "Epoch 84/100\n",
            "69958/69958 [==============================] - 46s 659us/step - loss: 5.4638e-06 - val_loss: 6.5606e-06\n",
            "Epoch 85/100\n",
            "69958/69958 [==============================] - 48s 688us/step - loss: 5.4667e-06 - val_loss: 6.4714e-06\n",
            "Epoch 86/100\n",
            "69958/69958 [==============================] - 48s 693us/step - loss: 5.5368e-06 - val_loss: 5.9815e-06\n",
            "Epoch 87/100\n",
            "69958/69958 [==============================] - 47s 665us/step - loss: 5.3394e-06 - val_loss: 6.0593e-06\n",
            "Epoch 88/100\n",
            "69958/69958 [==============================] - 47s 679us/step - loss: 5.3122e-06 - val_loss: 6.2395e-06\n",
            "Epoch 89/100\n",
            "69958/69958 [==============================] - 46s 662us/step - loss: 5.1877e-06 - val_loss: 5.8459e-06\n",
            "Epoch 90/100\n",
            "69958/69958 [==============================] - 48s 679us/step - loss: 5.1236e-06 - val_loss: 6.0527e-06\n",
            "Epoch 91/100\n",
            "69958/69958 [==============================] - 46s 661us/step - loss: 5.1869e-06 - val_loss: 5.6113e-06\n",
            "Epoch 92/100\n",
            "69958/69958 [==============================] - 49s 702us/step - loss: 5.4746e-06 - val_loss: 7.1990e-06\n",
            "Epoch 93/100\n",
            "69958/69958 [==============================] - 48s 686us/step - loss: 5.0939e-06 - val_loss: 5.6928e-06\n",
            "Epoch 94/100\n",
            "69958/69958 [==============================] - 46s 662us/step - loss: 4.9484e-06 - val_loss: 5.7874e-06\n",
            "Epoch 95/100\n",
            "69958/69958 [==============================] - 47s 678us/step - loss: 5.0235e-06 - val_loss: 5.3977e-06\n",
            "Epoch 96/100\n",
            "69958/69958 [==============================] - 46s 662us/step - loss: 4.7969e-06 - val_loss: 5.8874e-06\n",
            "Epoch 97/100\n",
            "69958/69958 [==============================] - 48s 681us/step - loss: 4.8600e-06 - val_loss: 6.4842e-06\n",
            "Epoch 98/100\n",
            "69958/69958 [==============================] - 48s 689us/step - loss: 4.9549e-06 - val_loss: 5.4634e-06\n",
            "Epoch 99/100\n",
            "69958/69958 [==============================] - 48s 684us/step - loss: 4.7131e-06 - val_loss: 5.9498e-06\n",
            "Epoch 100/100\n",
            "69958/69958 [==============================] - 47s 677us/step - loss: 4.7582e-06 - val_loss: 5.8292e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f046a318588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHFfJvCGetX",
        "colab_type": "text"
      },
      "source": [
        "#### 6. evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPqXQ_jVGiEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "861595f6-6849-4f42-806e-4939d669e591"
      },
      "source": [
        "predict_number=100\n",
        "Y_esitmated=model.predict(X_test[0:predict_number])\n",
        "fig=plt.figure()\n",
        "plt.plot(range(predict_number),Y_esitmated,'bo')\n",
        "plt.plot(range(predict_number),Y_test[0:predict_number],'ro')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjEK10n4NO2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9cc466f3-0b08-436f-9f6b-604edf01f14c"
      },
      "source": [
        "model.evaluate(X_test, Y_test, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29982/29982 [==============================] - 28s 933us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.82923933873379e-06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHv52DEZQLLa",
        "colab_type": "text"
      },
      "source": [
        "#### 7. conclusion\n",
        "The loss is small enough ..."
      ]
    }
  ]
}